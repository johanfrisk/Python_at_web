{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<h1>The First Page</h1>'\n",
      "b'<p>'\n",
      "b'If you like, you can switch to the'\n",
      "b'<a href=\"http://www.dr-chuck.com/page2.htm\">'\n",
      "b'Second Page</a>.'\n",
      "b'</p>'\n"
     ]
    }
   ],
   "source": [
    "# the beginning of a crawler...\n",
    "# find the url in the response and use that for the next step\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "fhand = urllib.request.urlopen('http://www.dr-chuck.com/page1.htm')\n",
    "for line in fhand:\n",
    "    print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing with Beautiful soup\n",
    "Beautifulsoup is already installed in the Anaconda package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - http://www.dr-chuck.com/page1.htm\n",
      "http://www.dr-chuck.com/page2.htm\n"
     ]
    }
   ],
   "source": [
    "# rewritten the code for Python 3... if fetches anchor tags\n",
    "\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = input('Enter - ')\n",
    "\n",
    "html = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# Retrieve a list of anchor tags\n",
    "# Each tag is like a dictionary of HTML attributes\n",
    "\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))\n",
    "    \n",
    "# test on this page: http://www.dr-chuck.com/page1.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing the first assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2482\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://python-data.dr-chuck.net/comments_371514.html'\n",
    "\n",
    "html = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# Retrieve a list of anchor tags\n",
    "# Each tag is like a dictionary of HTML attributes\n",
    "\n",
    "tags = soup('span')\n",
    "#print(tags)\n",
    "\n",
    "sum_of_tags = 0\n",
    "\n",
    "for tag in tags:\n",
    "    sum_of_tags += int(tag.contents[0])\n",
    "    \n",
    "print(sum_of_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example code to retrieve information\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    # Look at the parts of a tag\n",
    "    print('TAG:',tag)\n",
    "    print('URL:',tag.get('href', None))\n",
    "    print('Contents:',tag.contents[0])\n",
    "    print('Attrs:',tag.attrs)\n",
    "    \n",
    "def adding_series():\n",
    "    summ = 0\n",
    "    for i in range(11):\n",
    "        summ += i\n",
    "    return summ\n",
    "\n",
    "adding_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making a function out of the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2482"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://python-data.dr-chuck.net/comments_371514.html'\n",
    "\n",
    "def assign1(url):\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    tags = soup('span')\n",
    "    \n",
    "    sum_of_tags = 0\n",
    "    for tag in tags:\n",
    "        sum_of_tags += int(tag.contents[0])\n",
    "    return sum_of_tags\n",
    "    \n",
    "assign1(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Doing the second assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://python-data.dr-chuck.net/known_by_Fikret.html', 'http://python-data.dr-chuck.net/known_by_Montgomery.html', 'http://python-data.dr-chuck.net/known_by_Mhairade.html', 'http://python-data.dr-chuck.net/known_by_Butchi.html', 'http://python-data.dr-chuck.net/known_by_Anayah.html']\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "seed_url = 'http://python-data.dr-chuck.net/known_by_Fikret.html'\n",
    "urls = [seed_url]\n",
    "\n",
    "position = 3\n",
    "count = 4\n",
    "\n",
    "pos = position - 1\n",
    "\n",
    "for i in range(count):\n",
    "    html = urllib.request.urlopen(urls[-1]).read()\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    tags = soup('a')\n",
    "    urls.append(tags[pos].get('href', None))\n",
    "\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://python-data.dr-chuck.net/known_by_Shinay.html\n"
     ]
    }
   ],
   "source": [
    "# solution\n",
    "\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "seed_url = 'http://python-data.dr-chuck.net/known_by_Alan.html '\n",
    "urls = [seed_url]\n",
    "\n",
    "position = 18\n",
    "count = 7\n",
    "\n",
    "pos = position - 1\n",
    "\n",
    "for i in range(count):\n",
    "    html = urllib.request.urlopen(urls[-1]).read()\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    tags = soup('a')\n",
    "    urls.append(tags[pos].get('href', None))\n",
    "\n",
    "print(urls[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
